---
title: "INFSCI 2595 Fall 2020 Homework: 02"
subtitle: "Assigned August 31, 2020; Due: September 6, 2020"
author: "Shruti Gupta"
date: "Submission time: September 6, 2020 at 11:00PM EST"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Collaobrators

Include the names of your collaborators here.  

## Overview

This assignment introduces training and comparing models with the `caret` package. You will demonstrate a majority of the steps in predictive modeling applications. You will apply those actions to a simplified regression example. You will then work with a binary classification problem, focusing on calculating confusion matrix metrics from given model predictions.  

If you need help with understanding the R syntax please see the [R4DS book](https://r4ds.had.co.nz/) and/or the R tutorial videos and demos available on the [Canvas site](https://canvas.pitt.edu/courses/52857/pages/introduction-to-r-tutorials).  

**IMPORTANT**: code chunks are created for you. Each code chunk has `eval=FALSE` set in the chunk options. You **MUST** change it to be `eval=TRUE` in order for the code chunks to be evaluated when rendering the document.  

## Load packages

Load in the packages to be used in the first two problems.  

```{r, load_packages, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
```


## Problem 01

Last assignment, you were introduced to `R'`s formula interface. You will use the formula interface in this assignment. You will train and resample models using the `caret` package instead of calling the `lm()` directly.

You will use the `caret` package to train 9 polynomial models of various levels of complexity in Problem 01 and 02. You will specify a resampling scheme and a primary performance metric. The 9 models will be assessed through the resampled hold-out sets. The `caret` package will take care of the "book keeping" for you. That said, you will work through typing in the tedious formula interface for the different models. This is not the most efficient way to perform these actions. It is to get more practice with syntax.  

You will work with a synthetic data set. In Problem 01, the data are considered "noisy" while the data in Problem 02 has considerably less noise. You will try and interpret the influence noise has the model training process.  

The code chunk below reads in a data set for you. The code chunk below that displays a glimpse of the dataset which shows there are just 2 variables. The variable `x` is the input and the variable `y` is the response. You task is to predict `y` as a function of `x` using various polynomial models. As stated before, the data in Problem 01 are the "noisy" data.  

```{r, read_in_prob_01_data}
high_noise_github_file <- "https://raw.githubusercontent.com/jyurko/INFSCI_2595_Fall_2020/master/HW/02/prob_high_noise_dataset.csv"
prob_high_noise <- readr::read_csv(high_noise_github_file, 
                                   col_names = TRUE)
```

```{r, glimpse_prob_01_data}
prob_high_noise %>% glimpse()
```

### 1a)

**Use ggplot2 to create a scatter plot between the input, `x`, and the response, `y`. The scatter plot is created with the `geom_point()` geom.**  

**Does the relationship between the input and response appear to be linear or non-linear?**  

#### SOLUTION

```{r, solution_01a}
ggplot(data = prob_high_noise) + geom_point(mapping = aes(x = x, y = y))
```

Is the relationship linear or non-linear? \
The relationship appears to be linear

### 1b)

If you have not downloaded and installed `caret` please go ahead and do so now.  

**Load in the `caret` package with the `library()` function in the code chunk below.**  

#### SOLUTION

```{r, load_caret_package, warning=FALSE, message=FALSE}
library(caret)
```

### 1c)

The resampling scheme is specified by the `trainControl()` function in `caret`. The type of scheme is controlled by the `method` argument. For k-fold cross-validation, the number of folds is controlled by the `number` argument. You can decide to use repeated cross-validation by specifying the `repeats` argument to be greater than 1, as long as you specify the `method` argument to be `"repeatedcv"` instead of just `"cv"`.  

**Decide the type of resampling scheme you would like to use. Assign the result of the `trainControl()` function to the `my_ctrl` variable. Based on your desired number of folds and repeats, how many times will a model be trained and tested?**  

#### SOLUTION

```{r, solution_01c, eval=TRUE}
my_ctrl <- trainControl(method = "repeatedcv" , number = 10, repeats = 10)
```

How many times will an individual model be trained and tested?  
The individual model will be trained and tested 100 times. 

### 1d)

The response is a continuous variable.  

**You must select a primary performance metric to use to compare the models. Specify an appropriate metric to use for this modeling task. Choices must be written as a string and assigned to the `my_metric` variable. Possible are "Accuracy", "RMSE", "Kappa", "Rsquared", "MAE", "ROC". Why did you make the the choice that you did?**  

*NOTE*: Not all of the listed performance metrics above are relevant to regression problems!  

```{r, solution_011d, eval=TRUE}
my_metric <- 'Rsquared'
```

Why did you make your choice?  
`Rsquared` will tell the correlation between model and observation

### 1e)

You will now go through fitting 9 different models with the `train()` function from `caret`. You will use the formula interface to specify the model relationship. You must fit a linear (first order polynomial), quadratic (second order polynomial), cubic (third order polynomial), and so on up to and including a 9th order polynomial.  

**You must specify the `method` argument in the `train()` function to be `"lm"`. You must specify the `metric` argument to be `my_metric` that you selected in Problem 1d). You must specify the `trControl` argument to be `my_ctrl` that you specified in Problem 1c). Don't forget to set the `data` argument to be `prob_high_noise`.**  

**The variable names below and comments are used to tell you which polynomial order you should assign to which object.**  

*NOTE*: The models are trained in separate code chunks that way you can run each model apart from the others.  

#### SOLUTION

```{r, solution_01e_a, eval=TRUE}
### linear relationship
set.seed(2001)
mod_high_1 <- train(y ~ x , 
                    data = prob_high_noise, 
                    method = "lm", 
                    metric = my_metric, 
                    trControl = my_ctrl)
mod_high_1
```

```{r, solution_01e_b, eval=TRUE}
### quadratic relationship
set.seed(2001)
mod_high_2 <- train(y ~ x + I(x^2),
                    data = prob_high_noise,
                    method = "lm", 
                    metric = my_metric,
                    trControl = my_ctrl)
mod_high_2
```

```{r, solution_01e_c, eval=TRUE}
### cubic relationship
set.seed(2001)
mod_high_3 <- train(y ~ x + I(x^2) + I(x^3), 
                    data = prob_high_noise, 
                    method = "lm", 
                    metric = my_metric,
                    trControl = my_ctrl)
mod_high_3
```

```{r, solution_01e_d, eval=TRUE}
### 4th order
set.seed(2001)
mod_high_4 <- train(y ~ x + I(x^2) + I(x^3) + I(x^4), 
                    data = prob_high_noise,
                    method = "lm", 
                    metric = my_metric,
                    trControl = my_ctrl)
mod_high_4
```

```{r, solution_01e_e, eval=TRUE}
### 5th order
set.seed(2001)
mod_high_5 <- train(y ~ x + I(x^2)+ I(x^3) + I(x^4) + I(x^5),
                    data = prob_high_noise, 
                    method = "lm", 
                    metric = my_metric, 
                    trControl = my_ctrl)
mod_high_5
```

```{r, solution_01e_f, eval=TRUE}
### 6th order
set.seed(2001)
mod_high_6 <- train(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), 
                    data = prob_high_noise, 
                    method = "lm", 
                    metric = my_metric, 
                    trControl = my_ctrl)
mod_high_6
```

```{r, solution_01e_g, eval=TRUE}
### 7th order
set.seed(2001)
mod_high_7 <- train(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7), 
                    data = prob_high_noise,
                    method = "lm", 
                    metric = my_metric, 
                    trControl = my_ctrl)
mod_high_7
```

```{r, solution_01e_h, eval=TRUE}
### 8th order
set.seed(2001)
mod_high_8 <- train(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8), 
                    data = prob_high_noise,
                    method = "lm",
                    metric = my_metric,
                    trControl = my_ctrl)
mod_high_8
```

```{r, solution_01e_i, eval=TRUE}
### 9th order
set.seed(2001)
mod_high_9 <- train(y ~ x + I(x^2)+ I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9),
                    data = prob_high_noise,
                    method = "lm", 
                    metric = my_metric, 
                    trControl = my_ctrl)
mod_high_9
```

### 1f)

The code chunk below compiles all of the model training results for you. The `high_noise_results` object can be used to compare the models through tables and visualizations.  

```{r, assemble_resampling_high, eval=TRUE}
high_noise_results = resamples(list(fit_01 = mod_high_1,
                                    fit_02 = mod_high_2,
                                    fit_03 = mod_high_3,
                                    fit_04 = mod_high_4,
                                    fit_05 = mod_high_5,
                                    fit_06 = mod_high_6,
                                    fit_07 = mod_high_7,
                                    fit_08 = mod_high_8,
                                    fit_09 = mod_high_9))
```

**The `caret` package provides default visuals which rank the models based on their resampling hold-out results. Use the `dotplot()` function to create a dot plot with confidence intervals on the hold-out set performance metrics.**  

**You must create two plots. One for the metric you specified in `my_metric` and another with a second performance metric appropriate for a regression problem. You specify the metric to show in `dotplot()` with the `metric` argument.**  

**Based on your two figures, is there a clear best performing model? If so, which model is the best? If not, what are the top three models?**  

#### SOLUTION

```{r, solution_01f_a, eval=TRUE}
dotplot(high_noise_results , metric = my_metric)
```

```{r, solution_01f_b, eval=TRUE}
dotplot(high_noise_results , metric = 'RMSE')
```

Is there a best model?  \
According to me, the best model is fit_04, because it has low RMSE and high R-squared. fit_04 and fit_05 are almost same, but since 4th order polynomial is less complex than 5th order polynomial, I will consider 4th order the better.

### 1g)

The variable `mod_high_2` is a `caret` model object. However, you able to access the "underlying" model with `mod_high_2$finalModel` and use that object just like if we used the `lm()` function directly to fit the model. Therefore, regardless of your answer in Problem 1f), you will compare the coefficients of the top three models using the `coefplot::multiplot()` function. If you have not installed `coefplot` yet, please go ahead and do so.  

**Use `coefplot::multiplot()` to visualize the coefficients of your top three models by passing in three <caret object name>$finalModel objects into `coefplot::multiplot()`.**  

**Does anything stand out to you in the figure?**  

#### SOLUTION

```{r, solution_01g}
library(coefplot)
coefplot::multiplot(mod_high_4$finalModel , mod_high_5$finalModel, mod_high_6$finalModel)
```

Does anything stand out? \
the intercept for all the three model is same.\
As the model's coefficient degree increases, the value of coefficient decreases.
Example, for mod_high_4$finalModel , when the polynomial degree is x, it's value is around 5, when the polynomial degree is x^2 , value is around 2.5

## Problem 02

The data used in Problem 01 were generated using a "high" level of noise. You will now train and compare the same 9 models, but with data coming from a "low" level of noise. The underlying **data generating process** is the same between Problem 01 and Problem 02. All that changed is the noise level. We will learn what that means in more detail throughout this semester. For now, the main purpose is to compare what happens when you can train models under different noise level assumptions.  
The low noise level data are loaded in the code chunk below. The glimpse shows that the variable names are the same as those in the high noise level data from Problem 01.  

```{r, read_in_prob_02_data}
low_noise_github_file <- "https://raw.githubusercontent.com/jyurko/INFSCI_2595_Fall_2020/master/HW/02/prob_low_noise_dataset.csv"
prob_low_noise <- readr::read_csv(low_noise_github_file, col_names = TRUE)
```

```{r, glimpse_prob_02_data}
prob_low_noise %>% glimpse()
```

### 2a)

**Create a scatter plot between the input, `x`, and the response, `y`, for the low noise level data. How does this figure compare to the one you made in Problem 1a)?**  

#### SOLUTION

```{r, solution_02a}
ggplot(data = prob_low_noise) + geom_point(mapping = aes(x = x, y = y))
```

How does it compare?  
the figure in 1a), we cannot predict the true value of input as the data points are distributed. So, the range of true value points is more, that decreases the accuracy, but in 2a) we can predict the true value of input as the data points are very close to each other. Thus, the range of true value point is less and thus accuracy is more.

### 2b)

You will use the same resampling scheme and primary performance metric that you used in Problem 03 to train 9 different polynomial models. This time however you will use the low noise level data.  

**Train the 9 different polynomail models using the required formula interface, `method`, `trControl`, and `metric` arguments that you used in Problem 03. However, pay close attention and set the `data` argument to be `prob_low_noise`.**  

**The variable names and comments specify which polynomial to use.**  

*NOTE*: again this is VERY tedious...we will see more efficient ways of going through such a process later in the semester.  

#### SOLUTION

```{r, solution_02b_a, eval=TRUE}
### linear relationship
set.seed(2001)
mod_low_1 <- train(y ~ x , 
                   data = prob_low_noise,
                   method = "lm",
                   metric = my_metric,
                   trControl = my_ctrl)
```

```{r, solution_02b_b, eval=TRUE}
### quadratic relationship
set.seed(2001)
mod_low_2 <-train(y ~ x + I(x^2), 
                  data = prob_low_noise,
                  method = "lm",
                  metric = my_metric,
                  trControl = my_ctrl)
```

```{r, solution_02b_c, eval=TRUE}
### cubic relationship
set.seed(2001)
mod_low_3 <- train(y ~ x + I(x^2)+ I(x^3),
                   data = prob_low_noise,
                   method = "lm",
                   metric = my_metric,
                   trControl = my_ctrl)
```

```{r, solution_02b_d, eval=TRUE}
### 4th order
set.seed(2001)
mod_low_4 <- train(y ~ x + I(x^2) + I(x^3)+ I(x^4),
                   data = prob_low_noise,
                   method = "lm",
                   metric = my_metric,
                   trControl = my_ctrl)

```

```{r, solution_02b_e, eval=TRUE}
### 5th order
set.seed(2001)
mod_low_5 <- train(y ~ x + I(x^2)+ I(x^3) + I(x^4) + I(x^5),
                   data = prob_low_noise,
                   method = "lm",
                   metric = my_metric,
                   trControl = my_ctrl)
```

```{r, solution_02b_f, eval=TRUE}
### 6th order
set.seed(2001)
mod_low_6 <- train(y ~ x + I(x^2)+ I(x^3) + I(x^4) + I(x^5) + I(x^6),
                   data = prob_low_noise,
                   method = "lm",
                   metric = my_metric,
                   trControl = my_ctrl)
```

```{r, solution_02b_g, eval=TRUE}
### 7th order
set.seed(2001)
mod_low_7 <- train(y ~ x + I(x^2)+ I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7),
                   data = prob_low_noise,
                   method = "lm",
                   metric = my_metric,
                   trControl = my_ctrl)
```

```{r, solution_02b_h, eval=TRUE}
### 8th order
set.seed(2001)
mod_low_8 <- train(y ~ x + I(x^2)+ I(x^3)+I(x^4) + I(x^5) + I(x^6) + I(x^7)+I(x^8),
                   data = prob_low_noise,
                   method = "lm",
                   metric = my_metric,
                   trControl = my_ctrl)
```

```{r, solution_02b_i, eval=TRUE}
### 9th order
set.seed(2001)
mod_low_9 <- train(y ~ x + I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9),
                   data = prob_low_noise,
                   method = "lm",
                   metric = my_metric,
                   trControl = my_ctrl)
```

### 2c)

The code chunk below compiles all of the resampling results together for the models associated with the low noise level data.  

```{r, assemble_resampling_low, eval=TRUE}
low_noise_results = resamples(list(fit_01 = mod_low_1,
                                   fit_02 = mod_low_2,
                                   fit_03 = mod_low_3,
                                   fit_04 = mod_low_4,
                                   fit_05 = mod_low_5,
                                   fit_06 = mod_low_6,
                                   fit_07 = mod_low_7,
                                   fit_08 = mod_low_8,
                                   fit_09 = mod_low_9))
```

As with the `high_noise_results` object, you can now visualize summary statistics associated with the resampling results and identify the best performing models.  

**Create two dotplots again, using the same same two performance metrics you selected in Problem 1f). Is there a clear best model now? Did the order of the model performance change compared to what you saw with the high noise level data results?**  

#### SOLUTION

```{r, solution_02c_a, eval=TRUE}
dotplot(low_noise_results,metric = my_metric)
```

```{r, solution_02c_b, eval=TRUE}
dotplot(low_noise_results , metric = 'RMSE')
```

Is there a best model? Are the results different from the high level noise results?  \
Yes, in this the best model is **fit_05**.R squared is close to 1 ans RMSE is close to 0 which is considered to be the best model.
but in 1f), **fit_04** did great in high noise data but has performed poorly with low noise data.


### 2d)

You have now trained and assessed simple to complex models under low and high noise level conditions. As stated earlier, both datasets were generated from the same underlying **data generating process**.  

**Based on the results in Problem 01 and Problem 02, what impact do you think NOISE has on model training and the relationship to complexity?**  

#### SOLUTION

**What are your thoughts?** \
According to my understanding, if the data has less noise, then high complexity model perform well because then the outliers will be less and more complex model can fit the data, thus reducing the RMSE
but if the data has more noise, then low complexity model perform well because then the outliers will be more and high complexity model will not be able to handle such data and thus RMSE will be too high

## Problem 03

The code chunk below reads in a data set that you will work with in Problems 3, 4 and 5. A glimpse is printed for you which shows three variables, an input `x`, a model predicted event probability, `pred_prob`, and the observed output class, `obs_class`.  

```{r, read_binary_class_data}
model_pred_df <- readr::read_csv("https://raw.githubusercontent.com/jyurko/INFSCI_2595_Fall_2020/master/HW/02/hw02_binary_class.csv", col_names = TRUE)
```

```{r, glimpse_read_in_data}
model_pred_df %>% glimpse()
```

### 3a)

**Pipe the `model_pred_df` data set into the `count()` function to display the number of unique values of the `obs_class` variable.**  

#### SOLUTION

```{r, solution_03a}
model_pred_df %>% count(obs_class)
```

### 3b)

You should see that one of the values of `obs_class` is the event of interest and is named `"event"`.  

**Use the `mean()` function to determine the fraction of the observations that correspond to the event of interest. Is the data set a balanced data set?**  

#### SOLUTION

```{r, solution_03b}
mean(model_pred_df$obs_class == "event")

```

Is the data set balanced?  \
Yes, since the mean is .5, it is balanced

### 3c)

In lecture we discussed that regardless of the labels or classes associated with the binary response, we can encode the outcome as `y = 1` if the `"event"` is observed and `y = 0` if the `"non_event"` is observed. You will encode the output with this 0/1 encoding.  

The `ifelse()` function can help you perform this operation. The `ifelse()` function is a one-line if-statement which operates similar to the IF function in Excel. The basic syntax is:  

`ifelse(<conditional statement to check>, <value if TRUE>, <value if FALSE>)`  

Thus, the user must specify a condition to check as the first argument to the `ifelse()` function. The second argument is the value to return if the conditional statement is TRUE, and the second argument is the value to return if the conditional statement is FALSE.  

You can use the `ifelse()` statement within a `mutate()` call to create a new column in the `model_pred_df` data set.  

The code chunk below provides an example using the first 10 rows from the `iris` data set which is loaded into base R. The `Sepal.Width` variable is compared to a value of 3.5. If `Sepal.Width` is greater than 3.5 the new variable, `width_factor`, is set equal to `"greater than"`. However, if it is less than 3.5 the new variable is set to `"less than"`.  

```{r, show_ifelse_iris}
iris %>% 
  slice(1:10) %>% 
  select(starts_with("Sepal"), Species) %>% 
  mutate(width_factor = ifelse(Sepal.Width > 3.5, 
                               "greater than", 
                               "less than"))
```

You will use the `ifelse()` function combined with `mutate()` to add a column to the `model_pred_df` tibble.  

**Pipe `model_pred_df` into a `mutate()` call in order to create a new column (variable) named `y`. The new variable, `y`, will equal the result of the `ifelse()` function. The conditional statement will be if `obs_class` is equal to the `"event"`. If TRUE assign `y` to equal the value 1. If FALSE, assign `y` to equal the value 0. Assign the result to the variable `model_pred_df` which overwrites the existing value.**  

#### SOLUTION

```{r, solution_03c, eval=TRUE}
model_pred_df <- model_pred_df %>% mutate(y = ifelse(obs_class == "event", 1, 0))
```

### 3d)

You will now visualize the observed binary outcome as encoded by 0 and 1.  

**Pipe the `model_pred_df` object into `ggplot()`. Create a scatter plot between the encoded output `y` and the input `x`. Set the marker `size` to be 3.5 and the transparency (`alpha`) to be 0.5.**  

#### SOLUTION

```{r, solution_03d}
model_pred_df %>% ggplot(mapping = aes(x = x, y = y)) + geom_point(
                                        size = 3.5 ,
                                        alpha = 0.5)
```

### 3e)

The `model_pred_df` includes a column (variable) for a model predicted event probability.  

**Use the `summary()` function to confirm that the lower an upper bounds on `pred_prob` are in fact between 0 and 1.**  

#### SOLUTION

```{r, solution_03e}
model_pred_df %>% summary()
```

### 3f)

With the binary outcome encoded as 0/1 with the `y` variable we can overlay the model predicted probability on top of the observed binary response.  

**Use a `geom_line()` to plot the predicted event probability, `pred_prob`, with respect to the input `x`. Set the line `color` to be `"red"` within the `geom_line()` call. Overlay the binary response with the encoded response `y` as a scatter plot with `geom_point()`. Use the same marker size and transparency that you used for Problem 3d).**  

#### SOLUTION

```{r, solution_03f}


  model_pred_df %>% ggplot(mapping = aes(x=x,y=pred_prob)) +
  geom_point(mapping=aes(x = x, y=y),size=3.5,alpha=0.5)+
  geom_line(color="red") 
  
```


### 3g)

**Does the observed binary response "follow" the model predicted probability?**  

#### SOLUTION

Yes, observed binary response follow the model probability

## Problem 04

As you can see from the `model_pred_df` tibble, we have a model predicted probability but we do not have a corresponding classification.  

### 4a)

In order to classify our predictions we must compare the predicted probability against a threshold. You will use `ifelse()` combined with `mutate()` to create a new variable `pred_class`. If the predicted probability, `pred_prob`, is greater than the threshold set the predicted class equal to `"event"`. If the predicted probability, `pred_prob`, is less than the threshold set the predicted class equal to the `"non_event"`.  

**Use a threshold value of 0.5 and create the new variable `pred_class` such that the classification is `"event"` if the predicted probability is greater than the threshold and `"non_event"` if the predicted probability is less than the threshold. Assign the result to the new object `model_class_0.5`.**  

#### SOLUTION

```{r, solution_04a, eval=TRUE}
model_class_0.5 <- model_pred_df %>% mutate(pred_class = ifelse(pred_prob > .5,
                                                                "event",
                                                                "non_event"))
model_class_0.5
```

### 4b)

You should now have a tibble that has a model classification and the observed binary outcome.  

**Calculate the Accuracy, the fraction of observations where the model classification is correct.**  

#### SOLUTION

```{r, solution_4b}
accuracy_0.5 <- (53 + 35) / (6 + 31 + 53 + 35)
accuracy_0.5
```

### 4c)

We discussed in lecture how there are additional metrics we can consider with binary classification. Specifically, we can consider how a classification is correct, and how a classification is incorrect. A simple way to determine the counts per combination of `pred_class` and `obs_class` is with the `count()` function.  

**Pipe `model_class_0.5` into `count()` with `pred_class` as the first argument and `obs_class` as the second argument. You should see 4 combinations and the number of rows in the data set associated with each combination (the number or count is given by the `n` variable).**  

**How many observations are associated with False-Positives? How many observations are associated with True-Negatives?**  

#### SOLUTION

```{r, solution_04c}
model_class_0.5 %>% count(pred_class, obs_class)

```

your response here.  \

False-Positive - 6\
True-Negative - 53

### 4d)

**You will now calculate the Sensitivity and False Positive Rate (FPR) associated with the model predicted classifications based on a threshold of 0.5. This question is left open ended. You can calculate the Sensitivity and FPR however you see fit. Except you cannot use a model function library that performs the calculations automatically for you. Include as many code chunks as you feel are necessary.**  

#### SOLUTION

```{r, solution_04d}
sensitivity_.5 <- 35/(35+31)
sensitivity_.5
specificity_.5 = 53/(53+6)
specificity_.5
FPR_.5 <- 1 - specificity_.5
FPR_.5
```

### 4e)

We also discussed the ROC curve in addition to the confusion matrix. You will not have to calculate the ROC curve for many threshold values in this assignment. You will go through several calculations in order to get an understanding of the steps necessary to create an ROC curve.  

The first action you must perform is to make classifications based on a different threshold compared to the default value of 0.5, which we used previously.  

**Pipe the `model_pred_df` tibble into a `mutate()` function again, but this time determine the classifications based on a threshold value of 0.7 instead of 0.5. Assign the result to the object `model_class_0.7`.**  

#### SOLUTION

```{r, solution_04e, eval=TRUE}
model_class_0.7 <- model_pred_df %>% mutate(pred_class = ifelse(pred_prob > .7,
                                                                "event",
                                                                "non_event"))
model_class_0.7

                                            
```

### 4f)

**Perform the same action as in Problem 4e), but this time for a threshold value of 0.3. Assign the result to the object `model_class_0.3`.**  

#### SOLUTION

```{r, solution_04f, eval=TRUE}
model_class_0.3 <-  model_pred_df %>% mutate(pred_class = ifelse(pred_prob > .3,
                                                                "event",
                                                                "non_event"))
model_class_0.3
```

## Problem 5

You will continue with the binary classification application in this problem.  

### 5a)

**Calculate the Accuracy of the model classifications based on the 0.7 threshold.**  

#### SOLUTION

```{r, solution_05a}
accuracy_0.7 <- (26 + 58)/(26 + 1 + 40 + 58)
accuracy_0.7
```

### 5b)

**Calculate the Sensitivity and Specificity of the model classifications based on the 0.7 threshold. Again you can calculate these however you wish. Except you cannot use a model function library that performs the calculations automatically for you.**  

#### SOLUTION

```{r, solution_05b}
model_class_0.7 %>% count(pred_class, obs_class)
sensitivity_.7 <- (26/(26+40))
sensitivity_.7
specificity_.7 <- (58/(58+1))
specificity_.7
FPR_.7 <- 1-specificity_.7
FPR_.7
```

### 5c)

**Calculate the Accuracy of the model classifications based on the 0.3 threshold.**  

#### SOLUTION

```{r, solution_05c}
accuracy_0.3 = (48+41)/(48+41+18+18)
accuracy_0.3
```

### 5d)

**Calculate the Sensitivity and Specificity of the model classifications based on the 0.3 threshold. Again you can calculate these however you wish. Except you cannot use a model function library that performs the calculations automatically for you.**  

#### SOLUTION

```{r, solution_05d}
model_class_0.3 %>% count(pred_class, obs_class)
sensitivity_.3 <- 48/(48+18)
sensitivity_.3
specificity_.3 <- 41/(41+18)
specificity_.3
FPR_.3 <- 1 - specificity_.3
FPR_.3
```

### 5e)

You have calculated the Sensitivity and FPR at three different threshold values. You will plot your simple 3 point ROC curve and include a "45-degree" line as reference.  

**Use `ggplot2` to plot your simple 3 point ROC curve. You must compile the necessary values into a data.frame or tibble. You must use `geom_point()` to show the markers, `geom_abline()` with `slope=1` and `intercept=0` to show the reference "45-degree" line. And you must use  `coord_equal(xlim=c(0,1), ylim=c(0,1))` with your graphic. This way both axes are plotted between 0 and 1 and the axes are equal.**  

#### SOLUTION

```{r, solution_05e}


my_df <- data.frame(FPR = c(FPR_.3, FPR_.5, FPR_.7),
                        sensitivity = c(sensitivity_.3, sensitivity_.5,sensitivity_.7))

my_df %>% ggplot(aes(FPR, sensitivity))+
  geom_line(size = 1, alpha = .7)+
  geom_point(aes(FPR, sensitivity), size = 3) + 
  geom_abline(slope = 1 , intercept = 0)+
  coord_equal(xlim = c(0,1) , ylim = c(0,1))
  
```
